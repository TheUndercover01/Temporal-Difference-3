# Temporal-Difference


This repository is a submodule of another repository [TabularRL-Main](https://github.com/TheUndercover01/TabularRL-Main/tree/main), where the dynamics of the bot and environment, as well as the PyBullet simulation used, are explained in detail. 


## Overveiw

This repository contains implementations of four major algorithms based on the Temporal Difference method, drawing inspiration from the book "Reinforcement Learning: An Introduction" by Sutton and Barto. The algorithms implemented include Double Q-learning, Expected SARSA, Q-learning, and SARSA. Detailed explanations of these algorithms can be found in the accompanying Jupyter notebook file.

The repository comprises two main files: env.py and bot/robot.URDF. Within env.py, you'll find all the necessary methods required to run the environment and guide the bot through the essential steps during the simulation. On the other hand, bot/robot.URDF contains comprehensive information about the bot, encompassing its joints and links.

An image of the bot is provided below to give you an idea of its appearance.

Check out the bot on Onshape [here](https://cad.onshape.com/documents/04a8f06c4e82eef0aab52342/w/e26ea93d189b4fb4644d2868/e/ce0ae9d693e713171509edc4?renderMode=0&leftPanel=false&uiState=65b6963083efbe35d664705e).

<img src="https://github.com/TheUndercover01/TabularRL-Robotics/blob/main/image_bot.png?raw=true" alt="Robotic Arm" width="575" height="600">


## How to Run

